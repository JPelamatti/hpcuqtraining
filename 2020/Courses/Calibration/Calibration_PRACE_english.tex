% Copyright (C) 2018 - 2020 - Michael Baudin

  \documentclass{beamer}

%\setbeameroption{hide notes}
%\setbeameroption{show notes}
%\setbeameroption{show only notes}

  \include{macros}

\title[PRACE/UQ: Calibration]{
Calibration: deterministic and bayesian methods \\
PRACE training on High Performance Computing in Uncertainty Quantification 
}

\author[Baudin]{
M. Baudin
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{frame}
  \titlepage
  
  \begin{center}
\includegraphics[height=0.15\textheight]{figures/edf.jpg}
\end{center}

  \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Contents}
\tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deterministic code calibration}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Deterministic code calibration}

We consider a computer model $\vect{h}$ (i.e. a deterministic
function) to calibrate:

$$
       \vect{z} = \vect{h}(\vect{x}, \vect{\theta}),
$$

where

\begin{itemize}
\item  $\vect{x} \in \Rset^{d_x}$ is the input vector;
\item  $\vect{z} \in \Rset^{d_z}$ is the output vector;
\item  $\vect{\theta} \in \Rset^{d_h}$ are the unknown parameters of
   $\vect{h}$ to calibrate.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
Let $n \in \Nset$ be the number of observations. The standard
hypothesis of the probabilistic calibration is:

$$
       \vect{Y}^i = \vect{z}^i + \vect{\varepsilon}^i,
$$

for $i=1,...,n$ where $\vect{\varepsilon}^i$ is a random
measurement error such that:

$$
       E(\varepsilon)=\vect{0} \in \Rset^{d_z}, \qquad Cov(\varepsilon)=\Sigma \in \Rset^{d_z\times d_z},
$$

where $\Sigma$ is the error covariance matrix.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
The goal of calibration is to estimate $\vect{\theta}$, based on
observations of $n$ inputs
$(\vect{x}^1, \ldots, \vect{x}^n)$ and the associated $n$
observations of the output $(\vect{y}^1, \ldots, \vect{y}^n)$. 

In other words, the calibration process reduces the discrepancy between 
\begin{itemize}
\item  the observations $(\vect{y}^1, \ldots, \vect{y}^n)$ and 
\item  the predictions $\vect{h}(\vect{\theta})$. 
\end{itemize}

Given that $(\vect{y}^1, \ldots, \vect{y}^n)$ are realizations of a random
variable, the estimate of $\vect{\theta}$, denoted by
$\hat{\vect{\theta}}$, is also a random variable. 

Hence, the secondary goal of calibration is to estimate the distribution of
$\hat{\vect{\theta}}$ representing the uncertainty of the
calibration process.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
The standard observation model makes the hypothesis that the covariance
matrix of the error is diagonal, i.e.

$$
       \Sigma = \sigma^2 {\bf I}
$$

where $\sigma^2 \in \Rset$ is the constant observation error
variance.

In the remaining of this section, the input $\vect{x}$ is not
involved anymore in the equations. 

This is why we simplify the equation into:

$$
       \vect{z} = \vect{h}(\vect{\theta}).
$$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Least squares}

The residuals is the difference between the observations and the
predictions:

$$
       \vect{r}^i = \vect{y}^i - \vect{h}(\vect{\theta})^i
$$

for $i=1,...,n$. The method of least squares minimizes the square
of the euclidian norm of the residuals. This is why the least squares
method is based on the cost function $C$ defined by:

$$
       C(\vect{\theta}) = \frac{1}{2} \|\vect{y} - \vect{h}(\vect{\theta})\|^2 = \frac{1}{2} \sum_{i=1}^n \left( \vect{y}^i - \vect{h}(\vect{\theta})^i \right)^2,
$$

for any $\vect{\theta} \in \Rset^{d_h}$.


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
The least squares method minimizes the cost function $C$:

$$
\hat{\vect{\theta}} 
= \argmin_{\vect{\theta} \in \Rset^{d_h}} \frac{1}{2} \|\vect{y} - \vect{h}(\vect{\theta})\|^2.
$$

The unbiased estimator of the variance is:

$$
       \hat{\sigma}^2 = \frac{\|\vect{y} - \vect{h}(\vect{\theta})\|^2}{n - d_h}.
$$

Notice that the previous estimator is not the maximum likelihood
estimator (which is biased).
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Linear least squares}

In the particular case where the deterministic function $\vect{h}$
is linear with respect to the parameter $\vect{\theta}$, then the
method reduces to the linear least squares. 

Let $J \in \Rset^{n \times d_h}$ be the Jacobian matrix made of the
partial derivatives of $\vect{h}$ with respect to
$\vect{\theta}$:

$$
       J(\vect{\theta}) = \frac{\partial \vect{h}}{\partial \vect{\theta}}.
$$

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
Let $\vect{\mu} \in \Rset^{d_h}$ be a reference value of the
parameter $\vect{\theta}$. 

Let us denote by $J=J(\vect{\mu})$ the value of the Jacobian at the reference point
$\vect{\mu}$. 

Since the function is, by hypothesis, linear, the
Jacobian is independent of the point where it is evaluated. Since
$\vect{h}$ is linear, it is equal to its Taylor expansion:

$$
       \vect{h}(\vect{\theta}) = \vect{h}(\vect{\mu}) + J (\vect{\theta} - \vect{\mu}),
$$

for any $\vect{\theta} \in \Rset^{d_h}$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
The corresponding linear least squares problem is:

$$
       \hat{\vect{\theta}} = \argmin_{\vect{\theta} \in \Rset^{d_h}} \frac{1}{2} \|\vect{y} - \vect{h}(\vect{\mu}) + J (\vect{\theta} - \vect{\mu})\|^2.
$$

The Gauss-Markov theorem applied to this problem states that the
solution is:

$$
       \hat{\vect{\theta}} = \vect{\mu} + \left(J^T J\right)^{-1} J^T ( \vect{y} - \vect{h}(\vect{\mu})).
$$

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
The previous equations are the \emph{normal equations}. 

Notice, however, that the previous linear system of equations 
is not implemented as is, i.e.
we generally do not compute and invert the Gram matrix $J^T J$.

Alternatively, various orthogonalization methods such as the QR or the
SVD decomposition can be used to solve the linear least squares problem
so that potential ill-conditionning of the normal equations is
mitigated.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
This estimator can be proved to be the best linear unbiased estimator,
the \emph{BLUE}, that is, among the unbiased linear estimators, it is the one
which minimizes the variance of the estimator.

Assume that the random observations are gaussian:

$$
       \varepsilon \sim \mathcal{N}(\vect{0},\sigma^2 {\bf I}).
$$

Therefore, the distribution of $\hat{\vect{\theta}}$ is:

$$
       \hat{\vect{\theta}} \sim \mathcal{N}(\vect{\theta},\sigma^2 J^T J).
$$


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Non Linear Least squares}

In the general case where the function $\vect{h}$ is non linear
with respect to the parameter $\vect{\theta}$, then the resolution
involves a non linear least squares optimization algorithm. Instead of
directly minimizing the squared euclidian norm of the residuals, most
implementations rely on the residual vector, which lead to an improved
accuracy.

The difficulty in the nonlinear least squares is that, compared to the
linear situation, the theory does not provide the distribution of
$\hat{\vect{\theta}}$ anymore.

There are two practical solutions to overcome this limitation.
\begin{itemize}
\item bootstrap (randomly resample within the observations),
\item linearization (in the neighbourhood of $\hat{\vect{\theta}}$).
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Link with likelihood maximization}
Assume that the observation noise is gaussian with zero mean and constant 
variance $\sigma^2$: 
$$
\epsilon\sim \mathcal{N}(\bzero,\sigma^2 \imat),
$$ 
where $\sigma>0$ et $\imat\in\RR^{n\times n}$. 

This implies that the observations are independent. 

The likelihood of the i-th observation is:
$$
\ell(\by_i|\btheta,\sigma^2) = 
\frac{1}{\sqrt{2\pi \sigma^2}} 
\exp\left(-\frac{(y_i-H_i(\btheta))^2}{2 \sigma^2}\right)
$$
for $i=1,...,n$. 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
Since the observations are independent, the likelihood of the observations is 
the product:
$$
\ell(\by|\btheta,\sigma^2) = 
\prod_{i=1}^n \ell(\by_i|\btheta,\sigma^2)
$$
for $i=1,...,n$. 

This implies:
\begin{align*}
\log(\ell(\by|\btheta,\sigma^2)) 
&= -\frac{n}{2} \log(2\pi \sigma^2)
-\frac{\|\by-H(\btheta)\|_2^2}{2 \sigma^2}
\end{align*}
for any $\btheta\in\RR^p$ and $\sigma>0$. 

We maximize the likelihood with:
\begin{align*}
\hat{\btheta}
= arg min_{\btheta\in\RR^p} \frac{1}{2} \| H(\btheta) - \by\|_2^2
\end{align*}
and:
\begin{align*}
\hat{\sigma}^2
= \frac{1}{n} \| H(\hat{\btheta}) - \by\|_2^2.
\end{align*}

\end{frame}


\end{document}

