% Copyright (C) 2018 - 2020 - Michael Baudin

  \documentclass{beamer}

%\setbeameroption{hide notes}
%\setbeameroption{show notes}
%\setbeameroption{show only notes}

  \include{macros}

\title[PRACE/UQ: Calibration]{
Calibration: deterministic and bayesian methods \\
PRACE training on High Performance Computing in Uncertainty Quantification 
}

\author[Baudin]{
M. Baudin
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{frame}
  \titlepage
  
  \begin{center}
\includegraphics[height=0.15\textheight]{figures/edf.jpg}
\end{center}

  \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Contents}
\tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deterministic code calibration}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Deterministic code calibration}

We consider a computer model $\vect{h}$ (i.e. a deterministic
function) to calibrate:

$$
       \vect{z} = \vect{h}(\vect{x}, \vect{\theta}),
$$

where

\begin{itemize}
\item  $\vect{x} \in \Rset^{d_x}$ is the input vector;
\item  $\vect{z} \in \Rset^{d_z}$ is the output vector;
\item  $\vect{\theta} \in \Rset^{d_h}$ are the unknown parameters of
   $\vect{h}$ to calibrate.
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
Let $n \in \Nset$ be the number of observations. The standard
hypothesis of the probabilistic calibration is:

$$
       \vect{Y}^i = \vect{z}^i + \vect{\varepsilon}^i,
$$

for $i=1,...,n$ where $\vect{\varepsilon}^i$ is a random
measurement error such that:

$$
       E(\varepsilon)=\vect{0} \in \Rset^{d_z}, \qquad Cov(\varepsilon)=\Sigma \in \Rset^{d_z\times d_z},
$$

where $\Sigma$ is the error covariance matrix.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
The goal of calibration is to estimate $\vect{\theta}$, based on
observations of $n$ inputs
$(\vect{x}^1, \ldots, \vect{x}^n)$ and the associated $n$
observations of the output $(\vect{y}^1, \ldots, \vect{y}^n)$. 

In other words, the calibration process reduces the discrepancy between 
\begin{itemize}
\item  the observations $(\vect{y}^1, \ldots, \vect{y}^n)$ and 
\item  the predictions $\vect{h}(\vect{\theta})$. 
\end{itemize}

Given that $(\vect{y}^1, \ldots, \vect{y}^n)$ are realizations of a random
variable, the estimate of $\vect{\theta}$, denoted by
$\hat{\vect{\theta}}$, is also a random variable. 

Hence, the secondary goal of calibration is to estimate the distribution of
$\hat{\vect{\theta}}$ representing the uncertainty of the
calibration process.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
The standard observation model makes the hypothesis that the covariance
matrix of the error is diagonal, i.e.

$$
       \Sigma = \sigma^2 {\bf I}
$$

where $\sigma^2 \in \Rset$ is the constant observation error
variance.

In the remaining of this section, the input $\vect{x}$ is not
involved anymore in the equations. 

This is why we simplify the equation into:

$$
       \vect{z} = \vect{h}(\vect{\theta}).
$$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Least squares}

The residuals is the difference between the observations and the
predictions:

$$
       \vect{r}^i = \vect{y}^i - \vect{h}(\vect{\theta})^i
$$

for $i=1,...,n$. The method of least squares minimizes the square
of the Euclidian norm of the residuals. This is why the least squares
method is based on the cost function $C$ defined by:

$$
       C(\vect{\theta}) = \frac{1}{2} \|\vect{y} - \vect{h}(\vect{\theta})\|^2 = \frac{1}{2} \sum_{i=1}^n \left( \vect{y}^i - \vect{h}(\vect{\theta})^i \right)^2,
$$

for any $\vect{\theta} \in \Rset^{d_h}$.


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
The least squares method minimizes the cost function $C$:

$$
\hat{\vect{\theta}} 
= \argmin_{\vect{\theta} \in \Rset^{d_h}} \frac{1}{2} \|\vect{y} - \vect{h}(\vect{\theta})\|^2.
$$

The unbiased estimator of the variance is:

$$
       \hat{\sigma}^2 = \frac{\|\vect{y} - \vect{h}(\vect{\theta})\|^2}{n - d_h}.
$$

Notice that the previous estimator is not the maximum likelihood
estimator (which is biased).
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Linear least squares}

In the particular case where the deterministic function $\vect{h}$
is linear with respect to the parameter $\vect{\theta}$, then the
method reduces to the linear least squares. 

Let $J \in \Rset^{n \times d_h}$ be the Jacobian matrix made of the
partial derivatives of $\vect{h}$ with respect to
$\vect{\theta}$:

$$
       J(\vect{\theta}) = \frac{\partial \vect{h}}{\partial \vect{\theta}}.
$$

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
Let $\vect{\mu} \in \Rset^{d_h}$ be a reference value of the
parameter $\vect{\theta}$. 

Let us denote by $J=J(\vect{\mu})$ the value of the Jacobian at the reference point
$\vect{\mu}$. 

Since the function is, by hypothesis, linear, the
Jacobian is independent of the point where it is evaluated. Since
$\vect{h}$ is linear, it is equal to its Taylor expansion:

$$
       \vect{h}(\vect{\theta}) = \vect{h}(\vect{\mu}) + J (\vect{\theta} - \vect{\mu}),
$$

for any $\vect{\theta} \in \Rset^{d_h}$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
The corresponding linear least squares problem is:

$$
       \hat{\vect{\theta}} = \argmin_{\vect{\theta} \in \Rset^{d_h}} \frac{1}{2} \|\vect{y} - \vect{h}(\vect{\mu}) + J (\vect{\theta} - \vect{\mu})\|^2.
$$

The Gauss-Markov theorem applied to this problem states that the
solution is:

$$
       \hat{\vect{\theta}} = \vect{\mu} + \left(J^T J\right)^{-1} J^T ( \vect{y} - \vect{h}(\vect{\mu})).
$$

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
The previous equations are the \emph{normal equations}. 

Notice, however, that the previous linear system of equations 
is not implemented as is, i.e.
we generally do not compute and invert the Gram matrix $J^T J$.

Alternatively, various orthogonalization methods such as the QR or the
SVD decomposition can be used to solve the linear least squares problem
so that potential ill-conditioning of the normal equations is
mitigated.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
This estimator can be proved to be the best linear unbiased estimator,
the \emph{BLUE}, that is, among the unbiased linear estimators, it is the one
which minimizes the variance of the estimator.

Assume that the random observations are gaussian:

$$
       \varepsilon \sim \mathcal{N}(\vect{0},\sigma^2 {\bf I}).
$$

Therefore, the distribution of $\hat{\vect{\theta}}$ is:

$$
       \hat{\vect{\theta}} \sim \mathcal{N}(\vect{\theta},\sigma^2 J^T J).
$$


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Non Linear Least squares}

In the general case where the function $\vect{h}$ is non linear
with respect to the parameter $\vect{\theta}$, then the resolution
involves a non linear least squares optimization algorithm. Instead of
directly minimizing the squared Euclidian norm of the residuals, most
implementations rely on the residual vector, which lead to an improved
accuracy.

The difficulty in the nonlinear least squares is that, compared to the
linear situation, the theory does not provide the distribution of
$\hat{\vect{\theta}}$ anymore.

There are two practical solutions to overcome this limitation.
\begin{itemize}
\item bootstrap (randomly resample within the observations),
\item linearization (in the neighborhood of $\hat{\vect{\theta}}$).
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Link with likelihood maximization}
Assume that the observation noise is gaussian with zero mean and constant 
variance $\sigma^2$: 
$$
\epsilon\sim \mathcal{N}(\bzero,\sigma^2 \imat),
$$ 
where $\sigma>0$ et $\imat\in\RR^{n\times n}$. 

This implies that the observations are independent. 

The likelihood of the i-th observation is:
$$
\ell(\by_i|\btheta,\sigma^2) = 
\frac{1}{\sqrt{2\pi \sigma^2}} 
\exp\left(-\frac{(y_i-H_i(\btheta))^2}{2 \sigma^2}\right)
$$
for $i=1,...,n$. 
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
Since the observations are independent, the likelihood of the observations is 
the product:
$$
\ell(\by|\btheta,\sigma^2) = 
\prod_{i=1}^n \ell(\by_i|\btheta,\sigma^2)
$$
for $i=1,...,n$. 

This implies:
\begin{align*}
\log(\ell(\by|\btheta,\sigma^2)) 
&= -\frac{n}{2} \log(2\pi \sigma^2)
-\frac{\|\by-H(\btheta)\|_2^2}{2 \sigma^2}
\end{align*}
for any $\btheta\in\RR^p$ and $\sigma>0$. 

We maximize the likelihood with:
\begin{align*}
\hat{\btheta}
= arg min_{\btheta\in\RR^p} \frac{1}{2} \| H(\btheta) - \by\|_2^2
\end{align*}
and:
\begin{align*}
\hat{\sigma}^2
= \frac{1}{n} \| H(\hat{\btheta}) - \by\|_2^2.
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian code calibration}

\begin{frame}

The bayesian calibration framework is based on two hypotheses.

The first hypothesis is that the parameter $\vect{\theta}$ has a
known distribution, called the \emph{prior} distribution, and denoted by
$p(\vect{\theta})$.

The second hypothesis is that the output observations
$(\vect{y}^1, \ldots, \vect{y}^n)$ are sampled from a known
conditional distribution denoted by $p(\vect{y} | \vect{\theta})$.

For any $\vect{y}\in\Rset^{d_z}$ such that $p(\vect{y})>0$,
the Bayes theorem implies that the conditional distribution of
$\vect{\theta}$ given $\vect{y}$ is:

$$
p(\vect{\theta} | \vect{y}) = \frac{p(\vect{y} | \vect{\theta}) p(\vect{\theta})}{p(\vect{y})}
$$

for any $\vect{\theta}\in\Rset^{d_h}$.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}

The denominator of the previous Bayes fraction is independent of
$\vect{\theta}$, so that the posterior distribution is
proportional to the numerator:

$$
p(\vect{\theta} | \vect{y}) \propto  p(\vect{y} | \vect{\theta}) p(\vect{\theta}).
$$

for any $\vect{\theta}\in\Rset^{d_h}$.

In the Gaussian calibration, the two previous distributions are assumed
to be Gaussian.

More precisely, we make the hypothesis that the parameter
$\vect{\theta}$ has the Gaussian distribution:

$$
\vect{\theta} \sim \mathcal{N}(\vect{\mu}, B),
$$

where $\vect{\mu}\in\Rset^{d_h}$ is the mean of the Gaussian prior
distribution, which is named the \emph{background} and
$B\in\Rset^{d_h \times d_h}$ is the covariance matrix of the
parameter.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
Secondly, we make the hypothesis that the output observations have the
conditional gaussian distribution:

$$
\vect{y} | \vect{\theta} \sim \mathcal{N}(\vect{h}(\vect{\theta}), R),
$$

where $R\in\Rset^{d_z \times d_z}$ is the covariance matrix of the
output observations.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Posterior distribution}

Denote by $\|\cdot\|_B$ the Mahalanobis distance associated with
the matrix $B$ :

$$
\|\vect{\theta}-\vect{\mu} \|^2_B = (\vect{\theta}-\vect{\mu} )^T B^{-1} (\vect{\theta}-\vect{\mu} ),
$$

for any $\vect{\theta},\vect{\mu} \in \Rset^{d_h}$. Denote by
$\|\cdot\|_R$ the Mahalanobis distance associated with the matrix
$R$ :

$$
\|\vect{y}-H(\vect{\theta})\|^2_R = (\vect{y}-H(\vect{\theta}))^T R^{-1} (\vect{y}-H(\vect{\theta})).
$$

for any $\vect{\theta} \in \Rset^{d_h}$ and any
$\vect{y} \in \Rset^{d_z}$. Therefore, the posterior distribution
of $\vect{\theta}$ given the observations $\vect{y}$ is :

$$
   p(\vect{\theta}|\vect{y}) \propto \exp\left( -\frac{1}{2} \left( \|\vect{y}-H(\vect{\theta})\|^2_R 
   + \|\vect{\theta}-\vect{\mu} \|^2_B \right) \right)
$$

for any $\vect{\theta}\in\Rset^{d_h}$.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{MAP estimator}

The maximum of the posterior distribution of $\vect{\theta}$ given
the observations $\vect{y}$ is reached at :

$$
   \hat{\vect{\theta}} = \argmin_{\vect{\theta}\in\Rset^{d_h}} \frac{1}{2} \left( \|\vect{y} - H(\vect{\theta})\|^2_R 
   + \|\vect{\theta}-\vect{\mu} \|^2_B \right).
$$

It is called the \emph{maximum a posteriori} estimator or \emph{MAP}
estimator.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Regularity of solutions of the Gaussian Calibration}

The gaussian calibration is a tradeoff, so that the second expression
acts as a \emph{spring} which pulls the parameter $\vect{\theta}$
closer to the background $\vect{\mu}$ (depending on the "spring
constant" $B$, meanwhile getting as close a possible to the
observations. 

Depending on the matrix $B$, the computation may
have better regularity properties than the plain non linear least
squares problem.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Non Linear Gaussian Calibration : 3DVAR}

The cost function of the gaussian nonlinear calibration problem is :

$$
   C(\vect{\theta}) = \frac{1}{2}\|\vect{y}-H(\vect{\theta})\|^2_R 
   + \frac{1}{2}\|\vect{\theta}-\vect{\mu} \|^2_B
$$

for any $\vect{\theta}\in\Rset^{d_h}$.

The goal of the non linear gaussian calibration is to find the value of
$\vect{\theta}$ which minimizes the cost function $C$. In
general, this involves using a nonlinear unconstrained optimization
solver.

Let $J \in \Rset^{n \times d_h}$ be the Jacobian matrix made of
the partial derivatives of $\vect{h}$ with respect to
$\vect{\theta}$:

$$
J(\vect{\theta}) = \frac{\partial \vect{h}}{\partial \vect{\theta}}.
$$

If the covariance matrix $B$ is positive definite, then the
Hessian matrix of the cost function is positive definite 
(no matter of the rank of $J$). 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Linear Gaussian Calibration : bayesian BLUE}
We make the hypothesis that $h$ is linear with respect to
$\vect{\theta}$, i.e., for any
$\vect{\theta}\in\Rset^{d_h}$, we have :

$$
h(\vect{\theta}) = h(\vect{\mu}) + J(\vect{\theta}-\vect{\mu} ),
$$

where $J$ is the constant Jacobian matrix of $h$.

Let $A$ be the matrix:

$$
A^{-1} = B^{-1} + J^T R^{-1} J.
$$

We denote by $K$ the Kalman matrix:

$$
K = A J^T R^{-1}.
$$

The maximum of the posterior distribution of $\vect{\theta}$ given
the observations $\vect{y}$ is:

$$
\hat{\vect{\theta}} = \vect{\mu} + K (\vect{y} - H(\vect{\mu})).
$$

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
It can be proved that:

$$
   p(\vect{\theta} | \vect{y}) \propto 
   \exp\left(\frac{1}{2} (\vect{\theta} - \hat{\vect{\theta}})^T A^{-1} (\vect{\theta} - \hat{\vect{\theta}}) \right)
$$

for any $\vect{\theta}\in\Rset^{d_h}$.

This implies:

$$
\hat{\vect{\theta}} \sim \mathcal{N}(\vect{\theta},A)
$$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{General bayesian calibration}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{General bayesian calibration}

If the full posterior distribution is to be known, the 
general bayesian framework may be used. 

This allows to:
\begin{itemize}
\item use whatever (proper) prior distribution (no need to restrict to gaussian 
prior),
\item use whatever computer code $\vect{h}$ (no need to restrict to linear 
code).
\end{itemize}

One method is to create a Monte-Carlo Markov Chain ; the classical algorithm is then the 
Metropolis-Hastings algorithm. 

However, sampling from the posterior distribution 
requires to generate a large sample size, which is impractical 
when the $\vect{h}$ is costly. 

In this case, using a surrogate model is \emph{mandatory}. 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{References}

\begin{itemize}
\item  N. H. Bingham and John M. Fry (2010). Regression, Linear Models in Statistics, Springer Undergraduate Mathematics Series. Springer.
\item S. Huet, A. Bouvier, M.A. Poursat, and E. Jolivet (2004). Statistical Tools for Nonlinear Regression, Springer.
\item C.E. Rasmussen and C. K. I. Williams (2006), Gaussian Processes for Machine Learning, The MIT Press.
\end{itemize}

\end{document}

