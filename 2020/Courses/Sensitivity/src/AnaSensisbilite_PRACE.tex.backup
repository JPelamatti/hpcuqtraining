

\documentclass[8pt]{beamer}

\include{macros}
\usepackage{Math_Notations}
\usepackage{color}

\title[Sensitivity analyses]{Analyses de sensibilit\'e}

\author[Anne Dutfoy]{
 Anne DUTFOY
}

\institute[]{
 EDF R\&D PERICLES. \\
 anne.dutfoy@edf.Fr
}


\date[PRACE 2020]{PRACE 2020}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\AtBeginSection[]{
  \begin{frame}{Sommaire}
  \small \tableofcontents[currentsection, hideothersubsections]
  \end{frame}
}

  \begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[plain]
  \begin{columns}
    \column{0.5\textwidth}
    \column{0.5\textwidth}
    \titlepage
    \vfill
  \end{columns}
%\titlepage
\end{frame}

\setbeamertemplate{background canvas}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\begin{frame}
  \frametitle{Contexte}
\small
On considère
\alert{
\begin{align*}
 Y=f(\vect{X})
\end{align*}
}
\begin{itemize}
  \item $f$ est un {\bf modèle} (code de calcul, expression analytique, ...)
  \item $\vect{X}=(X_1, \dots, X_d)$ est l'ensemble  {\bf des paramètres incertains} que l'on modélise par une loi de probabilité multivari\'ee de dimension $d$
  \item $Y$ est la {\bf grandeur d'intérêt} évaluée par le modèle, supposée ici scalaire.
\end{itemize}

  \begin{block}{Pourquoi des \'etudes de sensibilit\'e?}
  Les principaux objectifs d'une \'etude de sensibilit\'e sont:
\begin{enumerate}
 \item \alert{\'eliminer des variables} peu voire pas influentes dans un contexte de grande dimension
 \item\alert{ hi\'erarchiser les variables} pour prioriser les efforts de mod\'elisation: il s'agit donc d'une quantification \emph{relative}: on veut que si $S_i < S_j$ alors $\Prob{\hat{S}_j \leq \hat{S}_i} \leq \varepsilon$
 \item \alert{quantifier l'influence d'une variable}: il s'agit donc d'une quantification \emph{exacte}: on veut que $\Prob{|\hat{S}_i - S_i| > \eta} \leq \varepsilon$
\end{enumerate}
\end{block}
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Sensibilit\'e: plusieurs notions}
\small

Plusieurs grandeurs peuvent quantifier la d\'ependance d'une variable \`a une autre.

\begin{block}{Sensibilit\'e = Dispersion = Variance}
Si l'on convient que la \alert{variance est une bonne quantification de la notion de dispersion}, les analyses de sensibilit\'e ont pour objectif de d\'eterminer les plus forts contributeurs \`a la variance de~$Y$.\\
On va donc s'int\'eresser \`a {\bf l'esp\'erance conditionnelle} $\Expect{Y|X_i} = Y^*_i$ qui la v. a. de l'espace engendr\'e par $X_i$ approchant $Y$ au mieux au sens des moindres carr\'es  :
\begin{align*}
Y^*_i = argmin_{g} \Expect{[Y-g(X_i))]^2}
\end{align*}
Pas de contrainte particuli\`ere sur la nature du lien entre $Y$ et $X_i$.\\
\vspace*{0.1cm}

On va donc \alert{comparer $\Var{Y^*_i}$} et \alert{$\Var{Y}$}:
 \begin{enumerate}
  \item dans le cas de variables ind\'ependantes: \alert{indices de Sobol},
  \item dans le cas de variables d\'ependantes: facteurs d'importance du \alert{cumul quadratique}, \alert{indices ANCOVA} .
 \end{enumerate}
\end{block}

\begin{block}{Sensibilit\'e = \emph{Distance} \`a l'ind\'ependance}
 Si $Y$ et $X_i$ sont tr\`es corr\'el\'ees, alors la copule de $(Y,X_i)$ est \emph{tr\`es \'eloign\'ee} de la copule ind\'ependante.\\
 Les \alert{mesures de divergence de Csiszar} permettent de quantifier cet \'eloignement entre lois.
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \begin{columns}
    \column{0.4\textwidth}
    \pgfuseimage{sommaire}

    \column{0.6\textwidth}
    {\usebeamercolor[fg]{title page}\huge{Sommaire}}
    \vspace{1cm}
    \small{\tableofcontents}
%\begin{center}
%     \includegraphics[width=0.5\textwidth]{figures/logo-ot}
%\end{center}
  \end{columns}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variables ind\'ependantes}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Indices de Sobol}


\begin{frame}
\frametitle{Indices de Sobol}
\small

\begin{block}{Décomposition de la variance}
De manière très générale, si $Y=f(\vect{X})$, avec $X$ \`a {\bf composantes ind\'ependantes}, alors on peut décomposer la variance comme suit :
\begin{equation}
\label{decompVar}
Var(Y) = \sum_i \Var{\Expect{Y|X_i}} + \sum_{i \neq j} \Var{\Expect{Y|X_i,X_j}} + \dots + \underbrace{\Var{\Expect{Y|X_1,\dots, X_n}}}_{=0}
\end{equation}
\end{block}


\begin{block}{Sobol indices}
L'\alert{\bf indice de Sobol d'ordre $k$} donne la part de variance de $Y$ expliqu\'ee par celle des $(X_{i_1},\dots, X_{i_k})$ exactement:

\begin{equation}
\label{SobolOrdrek}
\displaystyle S_{i_1, \dots, i_k} =\frac{\Var{\Expect{Y|X_{i_1}, \dots, X_{i_k}}}}{\Var{Y}}
\end{equation}

L'\alert{\bf indice total de Sobol d'ordre $k$} donne la part de variance  de $Y$ due aux entrées $(X_{i_1},\dots, X_{i_k}, \tilde{X})$:
\begin{equation}
\label{SobolTotalOrdrek}
\displaystyle S^T_{i_1, \dots, i_k} =\frac{\sum_I\Var{\Expect{Y|X_{I}}}}{\Var{Y}},\, \, \{i_1, \dots, i_k\} \subset I\subset \{1, \dots, n \}
\end{equation}

\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{D\'ecomposition de Hoeffding}
\small
La décomposition (\ref{decompVar}) de la variance de $Y$  r\'esulte de la d\'ecomposition de Hoeffding.

\begin{block}{Décomposition de Hoeffding d'une fonction intégrable sur $[0,1]^n$}
\textit{Si $f$ est intégrable sur $[0,1]^n$, elle admet une unique décomposition du type : }
\begin{equation}
\label{decompSobol}
\boldsymbol{ f(x_1, \dots, x_n) = f_0 + \sum_{i=1}^{i=n}f_i(x_i) + \sum_{1\leq i < j \leq n} f_{i,j} (x_i, x_j) + \dots + f_{1, \dots, n}(x_1, \dots, x_n)}
\end{equation}
\textit{où} $f_0 = cst$ \textit{et les fonctions de la décomposition sont orthogonales entre elles par rapport à la mesure de Lebesgue sur} $[0,1]^n$ :
\begin{equation}
\label{decompSobolCondOrthog}
 \int_0^1 f_{i_1, \dots, i_s}(x_{i_1}, \dots, x_{i_s})f_{j_1, \dots, j_k}(x_{j1}, \dots, x_{j_k}) d\vect{x} = 0
\end{equation}
\textit{dès lors que} $(i_1, \dots, i_s) \neq (j_1, \dots, j_k)$.
\end{block}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Sobol indices}
\small
Comment utiliser ce r\'esultat pour $Y = f(\vect{X})$ avec $\vect{X}$ vecteur al\'eatoire?


\begin{block}{Comment utiliser ce résultat ?}
    \small
      On aimerait décomposer le modèle $f$ selon la décomposition de Hoeffding \dots mais : \\
      \begin{enumerate}
      \item  \alert{\bf Les entrées de $f$ ne sont pas sur $[0,1]^n$} :  dans le cas général, $Y=f(\vect{X})$ où $\vect{X}$ est défini sur $\mathbb{R}$.
      \end{enumerate}
      \alert{$\Longrightarrow$} Si on pose
       \begin{equation}
         \label{phi}
         \vect{U} = (F_1(X_1), \dots, F_n(X_n))^t) = \phi^{-1}(\vect{X})
       \end{equation}
       alors on montre que $\vect{U}$ a une loi jointe de marginales uniformes et de copule celle de $\vect{X}$.\\
       En posant $Y = f(\vect{X}) = f \circ \phi (\vect{U})$, alors {\bf on peut utiliser la décomposition de Hoeffding sur $f \circ \phi$}.

       \begin{enumerate}
       \item  \alert{\bf Les indices de Sobol par rapport aux $U_i$ sont-ils les mêmes que ceux par rapport aux $X_i$ ?}
       \end{enumerate}
       \alert{$\Longrightarrow$} Rappel : Si $\vect{U} = \psi(\vect{X})$ où $\psi$ est un difféomorphisme et $Y=f(\vect{X})$ alors :

       \begin{equation}
         \label{varCond}
         \Expect{Y|\vect{U}} = \Expect{Y|\vect{X}}
       \end{equation}
       En effet : $\Expect{Y|\vect{U}} = \Expect{Y|\psi(\vect{X})}$ est le projeté orthogonal au sens $L_2$ de $Y$ sur l'espace engendré par $\psi(\vect{X})$, ie celui engendré par $\vect{X}$, d'où l'égalité des variables aléatoires (\ref{varCond}).\\
       Comme la transformation $\phi$ (\ref{phi}) agit composante par composante ($U_i \leftrightarrow X_i$) alors on a l'égalité des indicateurs de type :
       \begin{equation}
         \label{varCond2}
         \Var{\Expect{Y|U_{i_1}, \dots, U_{i_k}}} = \Var{\Expect{Y|X_{i_1}, \dots, X_{i_k}}}
       \end{equation}
       d'où {\bf l'égalité des indices de Sobol par rapport aux $U_i$ et aux $X_i$}.
  \end{block}

\end{frame}




\begin{frame}
  \frametitle{Sobol indices}
\small
  \begin{block}{Interprétation probabiliste de la décomposition de Hoeffding}
    \alert{\bf Supposons, sans perdre en généralité, que les variables $X_i$ soient à support $[0,1]$}. Alors en décomposant le modèle $f$ selon la décomposition de Sobol (\ref{decompSobol}), on écrit :
    \begin{equation}
      \label{decompSobolY}
      Y = f(\vect{X}) = f_0 + \sum_{i=1}^{i=n}f_i(X_i) + \sum_{1\leq i < j \leq n} f_{i,j} (X_i, X_j) + \dots + f_{1, \dots, n}(X_1, \dots, X_n)
    \end{equation}
    {\bf La condition d'orthogonalité (\ref{decompSobolCondOrthog}) des $f_{i_1, \dots, i_k}$  par rapport à la mesure de Lebesgue sur $[0,1]^n$ s'interprète comme un calcul d'espérance si les $X_i$ sont indépendantes}.\\
    \alert{$\Longrightarrow$} On suppose donc que dans la suite, les $X_i$ sont \alert{\bf indépendantes}.\\

    \alert{\bf Conclusion} : \\
    Y se décompose sous la forme d'une somme de {\bf variables aléatoires orthogonales} entre elles :
    \begin{equation}
      \label{decompSobolY2}
      \boldsymbol{ Y = f(\vect{X}) = Z_0 + \sum_{i=1}^{i=n}Z_i + \sum_{1\leq i < j \leq n} Z_{i,j}  + \dots + Z_{1,} \dots, n}
    \end{equation}
    où $Z_0 = cst$ et $Z_{i_1, \dots, i_s} \bot  Z_{j_1, \dots, j_k}$ (ie $\Expect{Z_{i_1, \dots, i_s}.Z_{j_1, \dots, j_k}}=0$).
  \end{block}
\end{frame}


\begin{frame}
  \frametitle{Sobol indices}
\small
  \begin{block}{Calcul des Indices de Sobol}
    \small{
      Grâce à la décomposition probabiliste (\ref{decompSobolY2}), on calcule $\Expect{Y}$ et $\Var{Y}$ aisément :
      $$
      \left\{
        \begin{array}{l}
          \Expect{Y}  =  Z_0 + \sum_{i=1}^{i=n}\underbrace{\Expect{Z_i}}_{=0 \mbox{ car } \bot Z_0} + \sum_{1\leq i < j \leq n} \underbrace{\Expect{Z_{i,j}}}_{=0 \mbox{ car } \bot Z_0}  + \dots + \underbrace{\Expect{Z_{1,} \dots, n}}_{=0 \mbox{ car } \bot Z_0}  \\[2em]
          \Expect{Y^2}   =   \sum_{I \neq J} \underbrace{\Expect{Z_IZ_J}}_{=0 \mbox{ car } \bot \mbox{ des } Z_I} + \sum_I \Expect{Z_I^2}  \sum_I \Expect{Z_I^2}
        \end{array}
      \right.
      $$
      \begin{eqnarray}
        \label{VarY}
        \Longrightarrow \boldsymbol{\Var{Y} = \sum_{i=1}^{i=n} V_i + \sum_{1\leq i < j \leq n} V_{i,j}  + \dots + V_{1, \dots, n}}
      \end{eqnarray}
      où $V_{i_1, \dots, i_k} = \Var{Z_{i_1, \dots, i_k}} = \Var{f_{i_1, \dots, i_k}(X_{i_1}, \dots, X_{i_k})}$.
    }
  \end{block}
\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Un exemple}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Un exemple}
\small
\underline{Analyse en aveugle d'une base de données de coefficients aérodynamiques}

\begin{block}{Donn\'ees}
  \begin{itemize}
  \item On s'intéresse à une fonction boîte noire de $\Rset^{24}$ dans $\Rset^{12}$
  \item Cette fonction est connue via une base de données de taille $n=377$
  \item Aucune information n'est connue sur la distribution des entrées constituant la base
  \item L'objectif est d'identifier pour chaque sortie les contributeurs principaux.
  \item \alert{On présente l'étude de la première sortie}
  \end{itemize}
  \end{block}

\begin{block}{Mode op\'eratoire}
  \begin{itemize}
  \item On teste l'hypothèse d'indépendance des entrées via leur corrélation des rangs: on ne peut pas rejeter l'hypothèse au seuil de 95\%
  \item On construit un méta-modèle par chaos polynomial pénalisé  sur 90\% de la base, on valide le méta-modèle sur les 10\% restant
  \item On exploite ce méta-modèle pour calculer les indices de Sobol et les indices de Sobol totaux du premier ordre.
  \end{itemize}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Qualité du méta-modèle}
  \begin{center}
    \resizebox{!}{4cm}{\includegraphics{Figures/Validation_Output_1_q_1.png}}

    Validation du modèle de chaos polynomial
  \end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Indices de Sobol}
\small
  \begin{center}
    \resizebox{!}{4cm}{\includegraphics{Figures/Sobol_Output_1.png}}

    Contributions des entrées à la variance de la sortie 1
  \end{center}
  La lecture de ce graphique montre qu'il est sans doute important de garder les entrées 6, 7, 8, 11, 21 et 23, et sans doute anodin de supprimer les entrées 3, 4, 5, 12, 13, 15, 16, 17, 18, 19, 20, 22 et 24, soit une division par au moins 2 de la dimension d'entrée.
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cas particuliers: mesures historiques}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Mesures historiques}
 \small
 
 Les indices de Sobol ont \'et\'e introduits par Sobol en 2001 (\cite{Sobol2001}). Des indices de sensibilit\'e existaient d\'ej\`a!:
 \begin{itemize}
  \item indices SRC, SRRC
  \item indices Pearson, Spearmann, PCC, PRCC
  \item facteurs d'importance issus du \emph{cumul quadratique}
 \end{itemize}
\alert{Dans le cas de composantes $X_i$ ind\'ependantes, ces indices sont des indices de Sobol.}

 \begin{block}{Si le modèle $f$ est affine : SRC}
  Si $\alert{Y = \alpha_0 + \sum_i \alpha_i X_i}$, avec les \alert{ $X_i$ indépendantes}, alors on définit le  \alert{ Standard Regression Coefficient (SRC)} :
    \begin{equation}
      SRC_i = \displaystyle \frac{\alpha_i^2 \Var{X_i}}{\Var{Y}}
    \end{equation}
    Donc {\bf $SRC $ est un indice de Sobol} à l'ordre 1 de $X_i$ : $\boldsymbol{    SRC (Y / X_i) = S_i(Y / X_i)}$.
 \end{block}

 
\begin{block}{Si le modèle $f$ est affine : Pearson}
 Si \alert{$Y=\alpha_0 + \sum_i \alpha_i X_i$}, on d\'efinit la \alert{ corr\'elation de Pearson} entre $Y$ et $X_i$ comme :
   \begin{equation}
    \rho(Y,X_i) =  \dfrac{{\rm cov}\left[Y,X_i \right]}{\sqrt{\Var{X_i}\Var{Y}}} = \dfrac{\Expect{[Y-\Expect{Y}][X_i - \Expect{X_i}]}}{\sqrt{\Var{X_i}\Var{Y}}}
  \end{equation}
De plus, si les \alert{ $X_i$ sont indépendantes}, alors on montre que :
  $$
  \rho(Y,X_i) = \dfrac{\alpha_i \Var{X_i}}{\sqrt{\Var{X_i}\Var{Y}}} \Longrightarrow (\rho(Y,X_i))^2 = SRC_i = S_i(Y/X_i)
  $$
  \end{block}
\end{frame}
 
 
 
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Mesures historiques}
 \small
  
 \begin{block}{Si le modèle $rang(f)$ est  affine par rapport aux $rang(X_i)$: SRRC}
   Si $Y = f(\vect{X})$ avec les \alert{$X_i$ indépendantes}, en posant $\vect{U} = (F_1(X_1), \dots, F_n(X_n))^t) = \phi^{-1}(\vect{X})$, on a $ Z = F_Y(Y) = F_Y \circ f \circ \phi (\vect{U})$. \\
   

 Si on suppose {\bf de plus} que
    \alert{\begin{align}\label{monotone}
           Z = \alpha_0 +\sum_i \alpha_i U_i
           \end{align}
           }
       alors on définit le \alert{ Standard Rank Regression Coefficient (SRRC)} :
          $$
     SRRC(Y/X_i) = SRC(Z / U_i) = \dfrac{\alpha_i^2 \Var{U_i}}{\Var{Z}} = S_i(Z/U_i)
    $$    
       
    Donc {\bf SRRC est un indice de Sobol} à l'ordre 1 calculé sur les rangs des $X_i$ et de $Y$.
           
  \end{block} 

   
 \begin{block}{Si le modèle $rang(f)$ est  affine par rapport aux $rang(X_i)$: Spearman}
  Si on suppose que  \alert{ les rangs sont lin\'eaires} (\ref{monotone}), on d\'efinit la \alert{ corr\'elation des rangs de Spearman} entre $Y$ et $X_i$ comme :
  $$
 \rho_S(Y,X_i) = \rho(F_Y(Y),F_i(X_i))
  $$
  De m\^eme, on montre que dans le cas de \alert{variables indépendantes} :
  $$
  (\rho_S(Y,X_i))^2 = SRRC(Y/X_i) = SRC(Z / U_i) = S(Z/U_i)
  $$
 
\end{block}
 \end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Mesures historiques}

\small
  Les facteurs d'importance issus du \emph{cumul quadratique} ont \'et\'e d\'efinis en m\'etrologie o\`u :
  \begin{itemize}
  \item $Y=f(\vect{X})$
   \item $\vect{X}$ est un vecteur gaussien \`a composantes ind\'ependantes {\bf peu dispers\'ees} ($\sigma / \mu \ll 1$)
   \item[$\Longrightarrow$] $f$ est lin\'earis\'ee autour de $\Expect{\vect{X}}$
  \end{itemize}


  \begin{block}{Cumul quadratique: approximation de Taylor à l'ordre 1 en $\Expect{\vect{X}}$}

$Y=f(\vect{X})$ est approché par son \alert{ approximation de Taylor à l'ordre 1 au point moyen } :
\begin{equation}
Y = f(\overline{\vect{X}}) + <\vect{\nabla }f(\overline{\vect{X}}), (\vect{X} - \overline{\vect{X}})> =  f(\overline{\vect{X}}) + \sum_i (X_i-\overline{X}_i)\left.\frac{\partial f}{\partial X_j}\right|_{\overline{\vect{X}}}
\end{equation}
Sous cette {\bf hypothèse de linéarité du modèle au point moyen} $\Expect{\vect{X}}$, et d'ind\'ependance des $X_i$, on calcule :
\begin{eqnarray}
\Var{Y} & = & \sum_i \left(\left.\frac{\partial f}{\partial X_j}\right|_{\Expect{\vect{X}}}\right)^2 \Var{X_i}
\end{eqnarray}
On définit le \alert{facteur d'importance de $X_i$} :
$$
\boldsymbol{
FI(X_i) = \left(\left.\frac{\partial f}{\partial X_j}\right|_{\Expect{\vect{X}}}\right)^2 \frac{\Var{X_i}}{\Var{Y}}
} = SRC(Y / X_i) = S_i(Y/X_i)
$$
{\bf Dans le cas des $X_i$ indépendantes et d'un modèle linéaire, les $FI$ du cumul quadratique sont des indices de Sobol à l'ordre 1}.
  
\end{block}

  \end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variables d\'ependantes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cumul quadratique}

%\begin{frame}
%\tableofcontents[currentsubsection]
%\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Cumul quadratique}
\small
Dans le cas o\`u les composantes $X_i$ sont d\'ependantes, {\bf on ne conserve de cette d\'ependance que la matrice de covariance} pour calculer :
\begin{itemize}
 \item les facteurs d'importance issus du cumul quadratique
 \item les indices ANCOVA
\end{itemize}

  \begin{block}{Cumul quadratique}
 $Y=f(\vect{X})$ est approché par son \alert{ approximation de Taylor à l'ordre 1 au point moyen } :
\begin{equation}
Y = f[\Expect{\vect{X}}] + <\vect{\nabla }f[\Expect{\vect{X}}], \vect{X} -\Expect{\vect{X}}> =  f[\Expect{\vect{X}}] + \sum_i [X_i-\Expect{X_i}]\left.\frac{\partial f}{\partial X_j}\right|_{\Expect{\vect{X}}}
\end{equation}
Sous cette \alert{ hypothèse de linéarité du modèle au point moyen} $\overline{\vect{X}}$, on calcule :
\begin{eqnarray}
\Var{Y} & = & \strut^t \vect{\nabla }f[\Expect{\vect{X}}].\mat{\rm Cov}\left[ \vect{X} \right]. \vect{\nabla }f[\Expect{\vect{X}}] = \sum_{i,j} \displaystyle \left.\frac{\partial f}{\partial X_i}\right|_{\Expect{\vect{X}}}{\rm Cov} \left[ X_i, X_j \right].\left.\frac{\partial f}{\partial X_i}\right|_{\Expect{\vect{X}}}
\end{eqnarray}
On définit le \alert{ facteur d'importance de $X_i$} :
\begin{equation}
\label{FI}
\boldsymbol{FI(X_i) = \displaystyle \frac{\left(\sum_{j} \left.\frac{\partial f}{\partial X_j}\right|_{\Expect{\vect{X}}}{\rm Cov} \left[ X_i, X_j \right]\right)  \left.\frac{\partial f}{\partial X_i}\right|_{\Expect{\vect{X}}}}{\Var{Y}}}
\end{equation}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Indices ANCOVA}

\begin{frame}
  \frametitle{ANCOVA indices}
\small

The ANCOVA (ANalysis of COVAriance) method, is a variance-based method generalizing the ANOVA (ANalysis Of VAriance) decomposition for models with correlated input parameters (see \cite{Caniou2012}).\\
It is based on the Hoeffding decomposition of $f$ that writes:
\begin{equation}
 Y = f(x_1, \dots, x_n) = f_0 + \sum_{U \subset \{ 1, n\}} f_U(\vect{X}_U)
\end{equation}
where $U$ is a non empty set of indices in $\{ 1, n\}$. Thus $f_U(\vect{X}_U)$ is the combined contribution of  $X_U$ to $Y$.

\begin{block}{Definition}
The total part of variance of $Y$ due to $\vect{X}_U$ writes:
\begin{align*}
 \alert{S_U = \dfrac{\Cov{Y, f_U(\vect{X}_U)}}{\Var{Y}} = S_U^{1} + S_U^{2}}
\end{align*}
 where
$$
 \left\{
 \begin{array}{lcl}
  S_U^{1} & = & \dfrac{\Var{f_U(\vect{X}_U)}}{\Var{Y}} \\[1em]
  S_U^{2} & = & \dfrac{\Cov{f_U(\vect{X}_U), \sum_{V | V \cap U = \emptyset} f_V(\vect{X}_V)}}{\Var{Y}}
  \end{array}
  \right.
$$

\alert{
$S_U^{1}$ is the contribution to $\Var{Y}$ of $\vect{X}_U$.\\
$S_U^{1}$ is the contribution to $\Var{Y}$ of $\vect{X}_U$ through its correlation to the other variables.
}
\end{block}

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extensions}

\subsection{Indices bas\'es sur la divergence de Csiszar}

\begin{frame}
 \frametitle{Divergence de Csiszar}
\small
\underline{Principe}: La sensibilit\'e de $Y$ \`a $X_i$ n'est plus d\'efinie par la part de la variance de $Y$ due \`a celle de $X_i$. C'est une notion de distance de la d\'ependance entre $Y$ et $X_i$ \`a l'ind\'ependance.\\
On suppose ici que $Y$ et $X_i$ sont scalaires, pour la clart\'e de l'expos\'e.

\begin{block}{Sensibilit\'e bas\'e sur la divergence de Csiszar}
 Dans \cite{Borgonovo2016} et \cite{DaVeiga2013} , les auteurs comparent la loi de $(X_i,Y)$, de densité $p_{X_i,Y}$ et la loi produit de $X_i$ et $Y$ (qui suppose l'ind\'ependance), de densit\'e $p_{Y}\otimes p_{X_i}$. \\
 Ils d\'efinissent des indices de sensibilité bas\'es sur une \alert{divergence de Csiszar $D_f$} par:
    \begin{align*}
      S_i^f= D_f(p_{Y \otimes X_i} \| p_{(Y,X_i)})
    \end{align*}
 On montre que cet indice : 
 \begin{itemize}
  \item est bas\'e sur la loi dans son ensemble et non juste sur ses moments
  \item est ind\'ependant des marginales (et donc d'une \'echelle)
 \end{itemize}
Il est bas\'e sur la copule seule:
    \alert{\begin{align*}
      S_i^f= D_f(\Pi \| c_{(Y,X_i)})
    \end{align*}}

\end{block}

\underline{Rappel}: la copule de $(X,Y)$ est \'egale \`a celle de $(f(X), g(Y))$ avec $f$ et $g$ croissantes.\\
En particulier, on les prend uniformes avec $f = F_X$ et $g = F_Y$.
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}
  \frametitle{Divergence de Csiszar}
\small
  \begin{block}{D\'efinition}
(\cite{Csiszar1963})
    Soient $P$ et $Q$ deux mesures de probabilité définies sur un espace $\Omega$ et $f$ une fonction convexe positive définie au minimum sur $\Rset^+$ telle que $f(1)=0$.\\
    La $f$-divergence de Csisz\'ar de $Q$ par rapport à $P$ est définie par:
    \begin{itemize}
     \item Si $P$ et $Q$ sont absolument continues par rapport \`a la mesure de Lebesgue $dx$, de densit\'es $p$ et $q$, et si $P  \ll Q$, alors: 
    \begin{align}
     \alert{ D_f(P||Q)=\int_{\Omega}f\left(\dfrac{p(x)}{q(x)}\right)q(x)dx} \quad \in[0,+\infty]
    \end{align}
    \item Si $P$ et $Q$ sont absolument continues par rapport \`a la mesure de comptage port\'ee par les $(x_k)_{k \in \Nset}$ (Dirac) et si $P  \ll Q$, alors:
    \begin{align}
     \displaystyle D_f(P||Q)=\sum_{k=0}^{\infty}f\left(\dfrac{p(x_k)}{q(x_k)}\right)q(x_k)     
    \end{align}

    \end{itemize}
    
    
    \end{block}
    \underline{Rappel}: $P  \ll Q$ signifie $q(x)= 0 \Longrightarrow p(x)=0$
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Exemples}
\small
\centering
      \begin{tabular}{lccc}
        \hline\noalign{\smallskip}
        Nom usuel & Formule & G\'en\'erateur $f(u)$ & $f(0)+f^*(0)$ \\
        \noalign{\smallskip}\hline\noalign{\smallskip}
        Variation totale & $\displaystyle\dfrac{1}{2}\int|p(x)-q(x)|dx$ & $\displaystyle\dfrac{1}{2}|u-1|$ & 1 \\
        Kullback-Liebler & $\displaystyle\int p(x)\log\dfrac{p(x)}{q(x)}dx$ & $\displaystyle-\log u$ & $\infty$ \\
        Hellinger (carr\'e) & $\displaystyle\int\left(\sqrt{p(x)}-\sqrt{q(x)}\right)^2dx$ & $\displaystyle\left(\sqrt{u}-1\right)^2$ & 2 \\
        Chi-2 Pearson & $\displaystyle\int\dfrac{\left(p(x)-q(x)\right)^2}{p(x)}dx$ & $\displaystyle(u-1)^2$ & $\infty$ \\
        \noalign{\smallskip}\hline \\[0.5em]
      \end{tabular}
 \\
 o\`u l'on pose $f^*:u\mapsto uf(1/u)$ la fonction $*$-conjugu\'ee de $f$
\begin{block}{Propri\'et\'es}
  \begin{itemize}
  \item Unicit\'e: $\forall (P,Q), D_{f_1}(P||Q)=D_{f_2}(P||Q) \Leftrightarrow \exists c\in\Rset, f_1(u)-f_2(u)=c(u-1)$ 
  \begin{itemize}
  \item Les divergences $D_{f_1}$ et $D_{f_2}$ quantifient les \'ecarts entre distribution de la m\^eme mani\`ere si $f_1$ et $f_2$ diff\`erent d'une fonction lin\'eaire de $(u-1)$
  \item les divergences bas\'ees sur Kullback-Liebler et Hellinger sont diff\'erentes
  \end{itemize}
  \item Sym\'etrie: $\forall (P,Q), D_{f}(P||Q)=D_{f^*}(Q||P)$ et $\forall (P,Q),D_{f^*}(P||Q)=D_{f}(P||Q)\Leftrightarrow \exists c\in\Rset, f^*(u)-f(u)=c(u-1)$
  \item Plage de valeurs: \alert{$\displaystyle 0=f(1)\leq D_f(P||Q)\leq f(0)+f^*(0)$}
  \item Convexit\'e: $\displaystyle \forall \lambda\in[0,1],\quad D_f(\lambda P_1+(1-\lambda)P_2||\lambda Q_1+(1-\lambda)Q_2)\leq\lambda D_f(P_1||Q_1)+(1-\lambda)D_f(P_2||Q_2)$
  \end{itemize}
  \end{block}

  \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Divergence de Csiszar}
\small
Les indices de sensibilité  s'\'ecrivent:
    \begin{align*}
      S_i^f= D_f(\Pi \| c_{(Y,X_i)}) = \int_{[0,1]^2}f\left(\dfrac{1}{c_{X_i,Y}(u,v)}\right)c_{X_i,Y}(u,v)\,dudv=\int_{[0,1]^2}f^*\left(c_{X_i,Y}(u,v)\right)\,dudv
    \end{align*}

  \begin{block}{Int\'erpr\'etation des indices}
  \begin{itemize}
   \item \alert{Si $Y \perp X_i$ alors $S_i^f=0$}  (\'equivalence si $f$ est strictement convexe). Dans ce cas, $X_i$ peut \^etre \'elimin\'e de l'\'etude car il n'influe pas sur $Y$.
   \item \alert{Si $Y=f(X_i)$ alors $S_i^f = f(0) + f^*(0)$}.
  \end{itemize}
La caractérisation de la plage de valeurs est le résultat principal pour l'analyse de dépendance.
  \end{block}

  \begin{block}{Challenge m\'ethodologiques et num\'eriques}
   Des recherches sont en cours pour les challenge suivants:
    \begin{itemize}
    \item comment se forger une opinion sur la valeur de l'indice? $S_i^f=0.8$: pour les indices de Sobol, cela signifie que $80\%$ de la variance de $Y$ est expliqu\'ee par la variance de $X_i$... mais pour Csiszar?
    \item quelle $f$ consid\'erer? Si $S_i^{f} > S_j^{f}$, a-t-on encore $S_i^{g} > S_j^{g}$? R\'eponse: non... donc, la hi\'erarchisation d\'epend de $f$. Il faut prendre un $f$ adapt\'e aux besoins. Par exemple, si $c(x_i, y)$ est petite dans certaines zones, on prend $f$ qui amplifie ces \'ecarts \`a 1 dans cette zone. Il faut donc se forger un savoir-faire!
      \item comment estimer une densit\'e de copule $c_{(Y,X_i)}$:  $\hat{S}_i^f = S_i^f(\hat{c})$ $\Longrightarrow$ utilisation de la copule de Bernstein?
     \item cr\'eer des tests d'ind\'ependance bas\'es sur un estimateur de $S_i^f$, en fonction de $f$: sous l'hypoth\`ese d'in\'d\'ependance, quel intervalle de confiance pour les valeurs de $\hat{S}_i^f$?
    \end{itemize}

  \end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Divergence de Csiszar - Test d'ind\'ependance}
  \small
  
  \begin{block}{Proposition I}
   Proc\'edure: 
   \begin{enumerate}
    \item Sur  un \'echantillon $k$ de $(x_i, y)$ de taille $n$, g\'en\'er\'e sous hypoth\`ese d'ind\'ependance entre $x_i$ et $y$, on reconstruit la densit\'e  $\hat{c}_k(x_i,y)$  de la copule de $(x_i,y)$ \`a l'aide de la copule de Bernstein (correctement param\'etr\'ee);
    \item On renouvelle l'op\'eration $N$ fois: on trace, en chaque point $(x_i,y)$, le quantile $5\%$ et $95\%$ des valeurs de $\hat{c}_k(x_i,y), 1\leq k \leq N$;
    \item On d\'elimite un \alert{domaine de confiance ponctuel de $90\%$}.
   \end{enumerate}
A partir de l'\'echantillon disponible, on trace la densit\'e de copule estim\'ee: si elle est en dehors du domaine, on rejette l'ind\'ependance.
  \end{block}

 \underline{Exemple}: Copule jointe estimée de $(X_{19}, Y_1)$ (gauche) et de $(X_8, Y_1)$ (droite)
\begin{center}
    \resizebox{5cm}{!}{\includegraphics{Figures/Output_1_input_19.png}}\, \resizebox{5cm}{!}{\includegraphics{Figures/Output_1_input_8b.png}}
  \end{center}
  Ces graphiques montrent qu'on ne peut pas rejeter l'hypothèse que $Y_1$ est indépendant de $X_{19}$, alors que $Y$ est manifestement fortement dépendant de $X_8$, cette dépendance étant essentiellement une co-monotonie croissante puisque le voisinage de la première diagonale reçoit l'essentiel de la masse probabiliste.
  \end{frame}
  
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Divergence de Csiszar - Test d'ind\'ependance}
  \small
  
  \begin{block}{Proposition II}
   Sur la proc\'edure pr\'ec\'edente, on calcule $\hat{S}_i ^f = S_i^j(\hat{c})$ pour chaque $f$ pour en calculer une distribution et d\'eterminer un intervalle de confiance de $\hat{S}_i ^f$ sous hypoth\`ese d'ind\'ependance.
  \end{block}

 \underline{Exemple}: Estimateur des indices de sensibilité, $n=1000$, $N = 10^4$ répétitions iid.
 \begin{center}
    \resizebox{4.3cm}{!}{\includegraphics{Figures/sensitivity_0.png}}\,
    \resizebox{4.3cm}{!}{\includegraphics{Figures/sensitivity_1.png}}\\[0.1em]
    \resizebox{4.3cm}{!}{\includegraphics{Figures/sensitivity_2.png}}\,
    \resizebox{4.3cm}{!}{\includegraphics{Figures/sensitivity_3.png}}\\[0.1em]
    
  \end{center}
  \end{frame}
  
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{R\'ef\'erences}

\begin{frame}[allowframebreaks]
  \frametitle{R\'ef\'erences}
  \bibliographystyle{abstract} % style alphabétique en anglais
  \bibliography{bibliothese} % pour afficher la biblio
\end{frame}



\end{document}


